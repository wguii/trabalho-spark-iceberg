{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Armzenamento e an\u00e1lise de dados um CSV utilizando PySpark + Apache Iceberg O CSV utilizado foi o: https://archive.ics.uci.edu/dataset/360/air+quality da UCI Machine Learning Repository, que cont\u00e9m dados sobre a qualiade do ar de uma cidade italiana medidas por um sensor de g\u00e1s categorizada por data e hora. Preparando ambiente Iniciamos a sess\u00e3o spark, definimos um DataFrame com os dados do CSV e ent\u00e3o tratamos os dados, dando aten\u00e7\u00e3o principalmente para o \"casting\" para int, double ou float das colunas para realizar opera\u00e7\u00f5es/compara\u00e7\u00f5es num\u00e9ricas. df = spark.read \\ .option(\"header\", True) \\ .option(\"inferSchema\", False) \\ .option(\"delimiter\", \";\") \\ .csv(\"data/sample.csv\") df = df.drop(\"_c15\", \"_c16\") df.printSchema() from pyspark.sql.functions import regexp_replace, col df = df.withColumn(\"T\", regexp_replace(col(\"T\"), r\"[^0-9.]\", \"\").cast(\"double\")) df = df.withColumn(\"T\", col(\"T\") / 10) df.show(5) df.printSchema() df.select(\"T\").show(20) Criamos ent\u00e3o a tabela Iceberg com: df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"local.sample_data\") Podemos agora, com o aux\u00edlio do Iceberg, utilizar comandos SQL para analisar e manipular os dados: Aqui, podemos realizar uma query selecionando apenas os registros onde a temperatura esteve acima dos 10 graus Celsius, representada pela coluna \"T\" no nosso dataframe. Exemplos de outros comandos SQL evidenciando as funcionalidades:","title":"Armzenamento e an\u00e1lise de dados um CSV utilizando PySpark + Apache Iceberg"},{"location":"#armzenamento-e-analise-de-dados-um-csv-utilizando-pyspark-apache-iceberg","text":"O CSV utilizado foi o: https://archive.ics.uci.edu/dataset/360/air+quality da UCI Machine Learning Repository, que cont\u00e9m dados sobre a qualiade do ar de uma cidade italiana medidas por um sensor de g\u00e1s categorizada por data e hora.","title":"Armzenamento e an\u00e1lise de dados um CSV utilizando PySpark + Apache Iceberg"},{"location":"#preparando-ambiente","text":"Iniciamos a sess\u00e3o spark, definimos um DataFrame com os dados do CSV e ent\u00e3o tratamos os dados, dando aten\u00e7\u00e3o principalmente para o \"casting\" para int, double ou float das colunas para realizar opera\u00e7\u00f5es/compara\u00e7\u00f5es num\u00e9ricas. df = spark.read \\ .option(\"header\", True) \\ .option(\"inferSchema\", False) \\ .option(\"delimiter\", \";\") \\ .csv(\"data/sample.csv\") df = df.drop(\"_c15\", \"_c16\") df.printSchema() from pyspark.sql.functions import regexp_replace, col df = df.withColumn(\"T\", regexp_replace(col(\"T\"), r\"[^0-9.]\", \"\").cast(\"double\")) df = df.withColumn(\"T\", col(\"T\") / 10) df.show(5) df.printSchema() df.select(\"T\").show(20) Criamos ent\u00e3o a tabela Iceberg com: df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(\"local.sample_data\")","title":"Preparando ambiente"},{"location":"#podemos-agora-com-o-auxilio-do-iceberg-utilizar-comandos-sql-para-analisar-e-manipular-os-dados","text":"Aqui, podemos realizar uma query selecionando apenas os registros onde a temperatura esteve acima dos 10 graus Celsius, representada pela coluna \"T\" no nosso dataframe. Exemplos de outros comandos SQL evidenciando as funcionalidades:","title":"Podemos agora, com o aux\u00edlio do Iceberg, utilizar comandos SQL para analisar e manipular os dados:"}]}